{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use gpu\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#import cifar10 dataset\n",
    "train_set = datasets.CIFAR10(root = \"Data\", train = True, download = True, transform = ToTensor())\n",
    "test_set= datasets.CIFAR10(root = \"Data\",train = False, download = True, transform = ToTensor() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#design the neural network\n",
    "\n",
    "def makethenet(printtoggle=False):\n",
    "    \n",
    "    class cifar_cnn(nn.Module):\n",
    "\n",
    "        def __init__(self,printtoggle):\n",
    "            #we call super here so as to inherit from nn.Moduule, which is the base class for  Neural networks\n",
    "            super().__init__()\n",
    "\n",
    "            #print toggle\n",
    "            self.print = printtoggle\n",
    "\n",
    "            ## --- feature map layers --- ##\n",
    "\n",
    "            # 1. convolution layer\n",
    "            self.conv1 = nn.Conv2d(3,64,3, padding = 1)   #returns a convoluting object which is later used for convolution\n",
    "            self.bnorm1 = nn.BatchNorm2d(64)\n",
    "\n",
    "            #2. convolution layer\n",
    "            self.conv2 = nn.Conv2d(64,128,3)              #returns a convoluting object\n",
    "            self.bnorm2 = nn.BatchNorm2d(128)\n",
    "\n",
    "            #2. convolution layer\n",
    "            self.conv3 = nn.Conv2d(128,256,3)              #returns a convoluting object\n",
    "            self.bnorm3 = nn.BatchNorm2d(256)\n",
    "\n",
    "            ## -- Linear layers -- ##\n",
    "            self.fc1 = nn.Linear(2*2*256,256)\n",
    "            self.fc2 = nn.Linear(256,64)\n",
    "            self.fc3 = nn.Linear(64,10)\n",
    "\n",
    "        def forward(self,x):\n",
    "\n",
    "            if self.print: print(f'Input: {list(x.shape)}')\n",
    "\n",
    "            ## --- first block(includes a set of conv, pooling and nn layers) --- ##\n",
    "\n",
    "            #apply max pooling after convolution\n",
    "            x = F.max_pool2d(self.conv1(x),2)\n",
    "            #apply leaky relu after batch normalising\n",
    "            x = F.leaky_relu(self.bnorm1(x))\n",
    "            if self.print :print(f'Second CPR Block: {list(x.shape)}')\n",
    "\n",
    "            ## --- second block: convolution -> maxpool -> batchnorm -> relu --- ##\n",
    "            x = F.max_pool2d(self.conv2(x),2)\n",
    "            x = F.leaky_relu(self.bnorm2(x))\n",
    "            if self.print: print(f'Second CPR block: {list(x.shape)}')\n",
    "\n",
    "            ## --- third block: convolution -> maxpool -> batchnorm -> relu --- ##\n",
    "            x = F.max_pool2d(self.conv3(x),2)\n",
    "            x = F.leaky_relu(self.bnorm3(x))\n",
    "            if self.print: print(f'Third CPR block: {list(x.shape)}')\n",
    "\n",
    "\n",
    "            # --- we need to change the shape so as to fit into the linear layers --- #\n",
    "            nUnits = x.shape.numel()/x.shape[0]          #numel() returns the total no. of elements in tensor        \n",
    "            x = x.view(-1,int(nUnits))                   #view() return a tensor with same data but different shape\n",
    "\n",
    "\n",
    "            # --- pass through the linear layer --- #\n",
    "            x = F.leaky_relu(self.fc1(x))\n",
    "            x = F.dropout(x,p=0.5,training=self.training)\n",
    "            x = F.leaky_relu(self.fc2(x))\n",
    "            x = F.dropout(x,p=.5,training=self.training) # training=self.training means to turn off during eval mode\n",
    "            x = self.fc3(x)\n",
    "            if self.print: print(f'Final output: {list(x.shape)}')\n",
    "\n",
    "            return x\n",
    "    \n",
    "    #create the model instance\n",
    "    net = cifar_cnn(printtoggle)\n",
    "    \n",
    "    #loss function \n",
    "    lossfun = nn.CrossEntropyLoss()\n",
    "    \n",
    "    #optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = 0.001, weight_decay = 1e-5)\n",
    "    \n",
    "    \n",
    "    return net,lossfun,optimizer     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_set,shuffle = True, batch_size = 64)\n",
    "test_loader = data.DataLoader(test_set,batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of X: <class 'torch.Tensor'>\n",
      "Input: [64, 3, 32, 32]\n",
      "Second CPR Block: [64, 64, 16, 16]\n",
      "Second CPR block: [64, 128, 7, 7]\n",
      "Third CPR block: [64, 256, 2, 2]\n",
      "Final output: [64, 10]\n",
      "\n",
      "Output size:\n",
      "torch.Size([64, 10])\n",
      " \n",
      "Loss:\n",
      "tensor(2.3112, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#test the model with one batch\n",
    "net,lossfun,optimizer = makethenet(True)\n",
    "\n",
    "X,y = next(iter(train_loader))\n",
    "print(f'type of X: {type(X)}')\n",
    "yHat = net(X)\n",
    "\n",
    "# check size of output\n",
    "print('\\nOutput size:')\n",
    "print(yHat.shape)\n",
    "\n",
    "# now compute the loss\n",
    "loss = lossfun(yHat,torch.squeeze(y))\n",
    "print(' ')\n",
    "print('Loss:')\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "\n",
    "def function2train():\n",
    "    \n",
    "    #number of epochs\n",
    "    num_pochs = 10\n",
    "    \n",
    "    #create a new model\n",
    "    net, lossfun, optimizer = makethenet()\n",
    "    \n",
    "    #send the model to the GPU\n",
    "    net.to(device)\n",
    "    \n",
    "    #initialize loses and accuracy\n",
    "    trainloss = torch.zeros(num_pochs)\n",
    "    trainacc = torch.zeros(num_pochs)\n",
    "    \n",
    "    #loop for each epoch\n",
    "    for epoch in range(num_pochs):\n",
    "        \n",
    "        # loop over training data batches\n",
    "        net.train() #switch to train mode\n",
    "        batchloss = []\n",
    "        batchacc = []\n",
    "        for X,y in train_loader:\n",
    "            \n",
    "            #push data to GPU\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            \n",
    "            #forwardpass and loss\n",
    "            yhat = net(X)\n",
    "            loss = lossfun(yhat,y)\n",
    "            \n",
    "            #backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            #loss and accuracy from this batch\n",
    "            batchloss.append(loss.item())\n",
    "            batchacc.append(torch.mean((torch.argmax(yhat,axis =1)==y).float()).item())\n",
    "            \n",
    "        # get average losses and accuracies across the batches\n",
    "        trainloss[epoch] = np.mean(batchloss)\n",
    "        trainacc[epoch] = 100*np.mean(batchacc)\n",
    "\n",
    "        print(trainloss)\n",
    "        print(trainacc)\n",
    "        \n",
    "        return trainloss,trainacc,net\n",
    "    \n",
    "y = function2train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
